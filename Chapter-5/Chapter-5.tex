\chapter{Code Architecture and Parallelization} \label{chapter:architecture-parallelism}

\section{Introduction}
\label{sect:introduction}
This chapter presents data management strategies, architecture and parallelization of Plume-SPH.
Simulation of volcanic plume require a careful definition of the domain of interest and multi-phase material involved in the flow, both of which change over time and involve transport over vast distances in a short time.
Computational strategies are developed to overcome these challenges by building mechanisms for efficient creation and deletion of particles for simulation, parallel processing (using MPI) and a dynamically defined halo domain (a domain that "optimally" captures all the material involved in the flow).
A background grid is adopted to reduce neighbor search costs and to decompose the domain. A Space Filing Curve (SFC) based ordering is
 used to assign unique identifiers to background grid entities. Time-dependent SFC based indices are assigned to particles to guarantee uniqueness of the identifier. Both particles and background grids are managed by hash tables which can ensure quick and flexible access. An SFC based three dimensional (3D) domain decomposition and a dynamic load balancing strategy are implemented to ensure good load balance. Several strategies are developed to improve performance: dynamic halo domains, calibrated particle weight and optimized workload check intervals. 
Finally, performace tests are carried out to check the effect of these strategies.

\begin{figure*}
\centering
\includegraphics[scale=0.43]{Chapter-5/Figures/Work_flow}
\hfil
\includegraphics[scale=0.43]{Chapter-5/Figures/Work_flow_adjust}
\caption{Workflow for SPH code. Figure to the left is the basic workflow. The right figure is the workflow that enables dynamic halo domain. These steps in orange box are newly added steps. Extra computational costs associated with these extra steps are shown to be neglectable in section \ref{sec:effect-of-halo-domain}}
\label{fig:Work_flow}
\end{figure*}

Figure \ref{fig:Work_flow} shows the basic workflow of our SPH code, which includes three sections in the major updating loop: pre-updating section (neighbour searching and updating wall boundary conditions), physical quantities updating section and domain repartition section. Adding of new eruption ghost particles is carried out right after particle position updating in the updating section. Pressure outlet boundary condition is setup at initial condition setup step and dose need to update during simulation. Data synchronization is carried out after every updating.

\section{Data management and parallelization}
\subsection{SFC based indexing}
Our data structure starts from assigning each particle and bucket an identifier, we refer to it as a key, which should be unique throughout the simulation. The key for a bucket is determined by the centroid coordinate of the bucket while the key for a particle is determined by adding coordinates and time step of the particle. The map from coordinates to key is based on SFC \citep{sagan2012space} which maps a n-dimensional domain to a one dimensional sequence. The standard procedure for obtaining the SFC is: 
\begin{itemize}
\item Scale all coordinates in computational domain ($\textbf{X}^\prime$) into an $n$ dimensional unit hypercube $[0,1]^n $: $\textbf{X}^\prime \rightarrow \textbf{X}$, where $\textbf{X}$ represents coordinates in the hypercube.
\item Compute $k_r = h_n(\textbf{X})$. Where $h_n$ is the map $h_n: [0,1]^n \rightarrow [0,1]$. 
\item Convert $k_r$ to integer $k$ by multiplying $k_r$ with a large integer and removing decimal part.
\item Sort all keys to form a SFC sequence. The SFC represents a curve passing through all particles (or centroids of buckets).
\end{itemize}
%
\begin{figure*}[!t]
\centering
\includegraphics[scale=0.26]{Chapter-5/Figures/SFC_particles_buckets}
\hfil
\includegraphics[scale = 0.275]{Chapter-5/Figures/SFC_particles_buckets_partition}
\caption{The left figure shows SFCs passing all particles and buckets. The right figure shows an example of a domain decomposition based on the SFC of buckets.}
\label{fig:SFC_domain_decomposition}
\end{figure*}

Details for constructing the map $h_n$ can be found in \citep{patra1995problem}. These keys denote a simple addressing and ordering scheme for the data, i.e., a global index space for all the objects.

In some situations, new particles need to be added during the simulation. For example, new particles need to be added at the bottom of the eject vent (see Fig. \ref{fig:Particle_adding_with_link}). To assign distinct identifiers for particles added at the "same position" but at different time steps, we extend the SFC-based key to a time-dependent SFC based key by including date of birth of particles into their keys. The time-dependent SFC based key can be denoted as: $[k,t]$, where $t$ is the time step. The map $h_n$ will become:
\begin{equation}
h_n: [0,1]^n \times \textbf{T} \rightarrow [0,1] \times \textbf{N}
\end{equation}
Where $\textbf{T} \subset [0,\infty)$ is the time dimension, and $\textbf{N}=\lbrace 0, 1, 2, 3...\rbrace$.
To guarantee locality, sorting of particle keys is based on $k$, that is to say, particles with smaller $k$ always come before particles with larger $k$. For particles that have the same $k$, ordering will then depend on $t$. Figure \ref{fig:SFC_domain_decomposition} shows SFC ordering of buckets and particles in buckets. 
Several features of this indexing scheme are suitable for SPH:
\begin{itemize}
\item Guaranteed uniqueness of keys.
\item The key of each object is generated purely based on its own coordinates. When we add new objects on different processes, the key of each object can be generated fast and independently.
\item Objects that are located closely in the Euclidian space will also be close to each other in the one dimensional SFC key space in a mean sense. Since SPH particles only interact with their neighbors, geometric locality can be exploited for efficient access of data.
\item This type of key effectively generates a global address space. Globalism of key and conservation of spatial locality make it easy to partition the sorted key sequence and obtain a decomposition of the problem.
\end{itemize}

The (time-dependent) SFC based indexing plays an pivot role in the architecture. Besides creating the unique ID efficiently and independently, SFC based domain decomposition helps preserve spatial locality making it very easy to design a hash function that preserve such locality in memory. The globalism of such indexing, which is also inherited in hash table, greatly simplifies complexity when moving elements around. As index generating (hence hashing) is independent and fast, creating/deleting of element are more efficient and flexible.
All these features help keep the overall architecture brief, robust and sustainable.

\subsection{Data management strategies}
\subsubsection{Particle and bucket}
The most basic data structure of SPH are particles and buckets. Both are defined as C++ classes. Information that is contained in particle objects can be categorized into six categories: identifier (the key), affiliate(rank of the process that the particle belongs to), primitive variables (variables in governing equations, e.g. density, velocity), secondary variables (physical properties that can be computed from primitive variables, they are stored to avoid repeated computations, e.g., pressure and temperature), flags (indicators, such as indicator for ghost particle or real particle) and neighbor information (it is a vector of particle keys in our application). Similarly, information that is contained in a bucket object can also be categorized into different categories: identifier, affiliate, dimension information (maximum and minimum coordinates, boundary information), flags (indicators, such as guest indicator, which is used to indicate whether the bucket belongs to another adjacent subdomain or not), neighbor information (keys of 27 neighbor buckets including its own key) and owned particles (it is a vector of particle keys in our application).
Usage of several independent flags creates a multiple-dimensional categorization space which enables flexible elements categorization simplifing algorithm design.
Objects of particles and buckets are then accessed through hash tables.

\subsubsection{Hash table and hash collision}
Several data structures have been considered for actual data storage. Vector and array can guarantee efficicent and flexible element access while inserting of new elements is either expensive or not allowed. List type of data structure allows flexible and efficient new element insert but element access is expensive ($(O(n)$).
Hash table guarantees fast element access ($(O(1)$) while allows flexible and efficient new element insert. Hashmap also allows flexible and efficient new element insert, but its element access is a little more expensive($(O(logn)$). We finally choose to use hash table, which is essentially a chunk of memory with many slots. The pointer to certain element object is stored in one slot. Based on the keys of data, the address-calculator(hashing) function determines in which slot the data should be stored:
\begin{equation}
Index = hash(key)
\end{equation}

Data in a hash table can be accessed with time complexity of $O(1)$ when there is no hash collision. One way to handle hash collisions is to use an additional sorted vector attached to the hash table. When several keys hash to the same index, a vector will be created. The vector is sorted based on keys so that a binary search (with an average time complexity of $O(log n)$) can be used to find the correct position for adding, deleting or retrieving of data. Another option is using an additional linked list which is more flexible in memory allocation but slower in searching $(O(n)$). In addition, pointer based memory accessing of linked lists will greatly slow down the efficiency of data access for long linked lists. Choosing the proper way to handle hash conflicts greatly depends on the problem itself. For volcano plume simulation, new particles are added at the bottom of the eruption vent (see Fig. \ref{fig:Particle_adding_with_link}), making the linked list a better choice. Pointers to these new particles will be stored in external linked lists as shown in Fig. \ref{fig:Particle_adding_with_link}.
\begin{figure}
\centering
{\includegraphics[scale=0.42]{Chapter-5/Figures/Particle_adding_with_link}}
{\caption{Non-uniform distribution of particles in the $[0,1]^n \times \textbf{T}$ space due to adding of new particles at a small area of the whole domain. Pointers to particles are managed by a hash table. External linked lists are attached to these slots with hash conflict.}
\label{fig:Particle_adding_with_link}}
\end{figure}

For time-independent keys, the hash function can be a simple function like:
\begin{equation}
Index= \frac{Key - Min\,Key}{Max\,Key - Min\,Key} 
\times Hash\,Table\,Size 
\label{eq:hash_function}
\end{equation}

One natural way to hash time-dependent keys $[k,t]$ is to convert the two elements in the key into one number taking $k$ as the higher digit and $t$ as the lower digit of the large number. However, for volcano plume simulation, only ghost particles for eruption boundary condition will be successively added at the same place: the bottom of the vent. That is to say, particles are distributed non-uniformly in the $[0,1]^n \times \textbf{T}$ space as shown in Fig. \ref{fig:Particle_adding_with_link}. To avoid a non-uniform sparse hash table and conserve spatial locality, we only plug the first number, $k$, of the key, $[k,t]$, into the hashing function, Eq. (\ref{eq:hash_function}). All particles with the same birth location will hash to the same slot and be handled by external linked lists.

\subsection{Domain decomposition and load balancing strategy}
\label{sect:load_balance}

\subsubsection{Calibrated particle workload} \label{sec:Weighted_work_load}
Particles in our problem can be categorized into four types according ghost-particle-flag: real particles, wall ghost particles, pressure ghost particles and eruption ghost particles. Ghost particles are for imposition of corresponding boundary conditions (See Fig. \ref{fig:bc_and_domain_decomp}). As different types of particles involve different amount of computational work, shown by table \ref{tab:Computational_cost_steps}, we assign different workload weights for different types of particles based on profiling data. Instead of simply using number of contained particles as workload for buckets, workload of each bucket is determined by summing up workload weights of all particles within the bucket.
\begin{equation}
W^{bucket} = \sum_i^{N^{particle}} W_i^{particle}
\label{eq:work-load-bucket}
\end{equation}
where $N^{particle}$ is total number of particles contained by the bucket. $W_i^{particle}$ is weight of particle, depending on particle type (see Table \ref{tab:Computational_cost_steps}).
The SFC of buckets now becomes a weighted sequence. Domain decomposition is conducted by cutting the weighted SFC of buckets into pieces.

\begin{table}
\centering
\caption{Computational cost per particle for different steps}	
\label{tab:Computational_cost_steps}
	  \begin{tabular}{lrrrrrr}
	    \hline
	    Step & Cost ($ms$) & Real & wall & eruption & pressure\\
	    \hline
	    neighbor search & 0.41 & Yes & No & No & No \\
	    update momentum and energy & 0.70 & Yes & No & No & No\\
	    update density & 0.42 & Yes & No & No & No \\
	    update position & 0.01 & Yes & No & Yes &  No\\
	    velocity filtering& 0.43 & Yes & No & No & No\\
	    apply wall boundary condition     & 0.75 & No & Yes & No & No\\
	    summation ($ms$) & - & 1.97 & 0.75 & 0.01 & 0.00\\
	    \hline
	  \end{tabular}
\end{table}

\subsubsection{Domain decomposition and dynamic load balancing}
Figure \ref{fig:SFC_domain_decomposition} shows how the domain is decomposed based on partition of the SFC of buckets. SFC maps buckets in 3D space into a 1D sequence with spatial locality well preserved. Splitting of the SFC automatically divides buckets into groups. The particles inside the buckets are automatically split into several groups along with buckets that contain them. Each piece of the SFC after partitioning, which will be calculated by each processor, should then have a balanced total workload. That is to say:
\begin{equation}
W^{process}_1 \approx W^{process}_2 ... \approx W^{process}_i ...\approx W^{process}_{N^{process}}
\label{eq:work-load-balance}
\end{equation}
where $N^{process}$ is total number of processors, workload of each processor is calculated according to Eq. (\ref{eq:work-load-process}).
\begin{equation}
W^{process} = \sum_i^{N^{bucket}} W_i^{bucket}
\label{eq:work-load-process}
\end{equation}
where the workload of each bucket is evaluated based on Eq.(\ref{eq:work-load-bucket}).

As some of the neighbor particles reside in other partitions, a set of ``guest'' particles and buckets are used to synchronize data across different partitions. To minimize communications, data is synchronized only after updating of physical quantities and positions or after repartition. Data associated with a single particle (or bucket) are first packed up before sent out and then unpacked after receiving. In addition, not all information contained by element (bucket or particles) is packed. For example, secondary variables and neighbour information of particles are not send out. In this way, the amount of data that need communicate is reduced. Packed data need to be delivered to the same processor will be loaded onto a bus (an array whose elements are packed buckets or particles) and send out all together, which reduces frequency of communication and hence communication overhead. Non-blocking MPI communications is adopted to reduce communication overhead.

Movement of particles, adding of new particles, and adjustments of domain will lead to significant load imbalance between processes. To restore load balance, the computational load is monitored at a given interval which is optimized based on numerical experiments (see details in section \ref{sec:numerical-tests-computation}). Repartitioning is carried out when load imbalance is larger than a given tolerance.

\section{Dynamic halo domain} 
For plume simulation and similar phenomena, where some fluid ejects into a stationary fluid and gets mixed, the stationary fluid must be represented.
A lot of CPU time will be spent on computation associated with these stationary particles. If simulation of stationary particles can be avoided, the computational cost will be greatly reduced.
We propose a simple strategy to add such a feature to our code with low computational cost. We add a scan function to monitor the outermost layer of the domain. When the ejected fluid reaches the boundary of the current domain, pressure ghost particles (light blue particles in the middle picture of Fig. \ref{fig:bc_and_domain_decomp}) will be turned to real particles (red particles in the middle picture of Fig. \ref{fig:bc_and_domain_decomp}) and then add a new layer of pressure ghost particles for pressure boundary conditions. Wall ghost particles (grey particles in the middle picture of Fig. \ref{fig:bc_and_domain_decomp}) will also be added when necessary. The original workflow is modified to enable such a feature. The modified workflow is shown on the right in Fig. \ref{fig:Work_flow}). These extra steps, as shown in Table \ref{tab:Computational_cost_doamin_adj} have ignorable overheads.

An involvement flag is added to particle class to distinguish different states of involvement. Particles are categorized into three groups based on the value of the involvement flag: involved, potentially involved, and not involved. The involved particles are particles that have already been affected by the mixing. The potentially involved particles are particles that have not been involved in mixing but are adjacent to involved particles and will thus be involved in the near future. 
All communication and computation associated with uninvolved particles can be ignored. That is to say, only potential involved and involved particles need to be simulated.
As simulation progresses, the ejected fluid will reach a larger area and more and more particles will be influenced. When originally stationary air is influenced by erupted material, the mass fraction of the erupted material will increase from zero to a positive value. So we can determine whether a particle should turn to involved status based on whether the mass fraction of that particle is larger than a given threshold ($10^{-5} $ in our simulation) or not. Based on such criteria, any real particles of phase 2 (erupted material) are qualified to be involved particles.  Particles of phase 1 (air) are also qualified to be involved particles if they are inside of the plume or close to the margin of plume. Other physical properties, like velocity, might also serve as alternative ``switch criteria''. Switching of particles from potentially involved to involed status is carried out right after mass fraction updating (the ``density update" step in left figure of Fig. \ref{fig:Work_flow}).

A similar scheme is also deployed for buckets resulting in a categorization of buckets. The additional complexity is that potentially involved ghost particles must also be taken into accounted. The switch criteria for bucket from potentially involved to involved is based on the fact whether the bucket contains any involved particles. Switching of bucket from potentially involved to involved is carried out in the a newly added step in the workflow, the ``Scan the Most Outside Layer
Buckets \& Return Adapt Indicator" step.

By implementing the algorithm of dynamics halo domain presented above, the computational domain grows dynamically during simulation. Figure \ref{fig:bc_and_domain_decomp} shows a cross-sectional view of a typical computational domain during simulation. As can be seen, the plume just fit into the computational domain, which has irregular geometry.

\section{Numerical test} \label{sec:numerical-tests-computation}
Numerical tests are carried out in this section. Tests on overall parallel performance, including the weak scalability and strong scalability, are presented first. Then the effects, in terms of computational efficiency, of several special algorithms are demonstrated, including the optimized load balance check interval, calibrated particle weights and the dynamic halo domain.

\subsection{Scalability test}
\begin{figure*}[!t]
\centering
\includegraphics[scale=0.31]{Chapter-5/Figures/strong_scale}
\hfil
\includegraphics[scale=0.31]{Chapter-5/Figures/strong_scale_zoom}
\hfil
\includegraphics[scale=0.31]{Chapter-5/Figures/weak_scale}
\caption{The left figure shows strong scalability tests result. middle figure is the zoomed view of first one. It is obviously shown that strong scalability is better when the problem size is larger. The right figure is weak scalability test results}
\label{fig:2cases_efficiency}
\end{figure*}

Experiments have been carried out on the computational cluster of Center for Computational Research (CCR) at Buffalo. 
The compute servers have 12 cores (two Xeon E5645 processors per server) running at 2.40GHz clock rate with 4GB memory per core on a Q-Logic Infiniband network. Main memory and level 3 cache are shared on each node. The simulation domain is a box with initial dimension $[-4.8km, 4.8km]\times [-4.8km, 4.8km] \times [0km, 6km]$ for case 1 and $[-10km, 10km]\times [-10km, 10km] \times [0km, 5km]$ for case 2. The smoothing length (we set initial intervals between particles equal to smoothing length) equals to 200m and 100m respectively for test case 1 and test case 2. So the computational workload of test case 2 is larger than that of the test case 1. The simulations run for 20s physical time.  Parallel speed up is shown in Fig. \ref{fig:2cases_efficiency}. Test case 2 shows better speed up than test case 1 which implies that strong scalability can be improved by increasing total amount of workload.
The weak scalability test is conducted with the same initial domain as test case 1 and various smoothing length. Each simulation runs for 400 time steps. The average number of real particles for each process keeps constant at $25900$, so the average workload for each processor is approximately constant. 
Right side figure in Fig \ref{fig:2cases_efficiency} shows the timing of a weak scalability test.

\subsection{Effect of calibrated particle weights} \label{sec:effect-of-calibrated-particle-weights}

\begin{table}
\centering
      \caption{Simulation time when using the same particle weight and different particle weights.} 
	  \begin{tabular}{lrrrr}
	    \hline
	    Physical time & 10 s & 20s & 30 s & 40 s \\
	    \hline
	    Same weight & 1141.7 & 4119.4 & 10371.0 & 12453.7\\
	    Different weights & 1108.2 & 4057.0 & 10281.5 & 12166.3\\
	    \hline
	  \end{tabular}
	  \label{tab:same_diff_particle_weight}
\end{table}

In section \ref{sect:load_balance}, we proposed to use different particle weights for different types of particles when estimate the workload of buckets. The effect of using different particle weight is demonstrated in table \ref{tab:same_diff_particle_weight}. The simulation time is reduced by using different particle weights. However, the amount of time that is reduced is not as significant as we expected. One possible explanation is that real particles are the major population, so the load imbalance caused by assigning improper weight values to ghost particles is small.

\subsection{Effect of workload check interval} \label{sec:effect-of-workload-check-interval}
\begin{figure}
\centering
{\includegraphics[scale=0.45]{Chapter-5/Figures/int_bar}}
{\caption{The influence of different load balance check intervals on simulation time. The bar values are the extra simulation time in log scale. The optimized interval is 3s.}
\label{fig:check_int}}
\end{figure}
The best load balance check interval is calibrated based on a series of simulations with different load balance scan intervals. From 0 - 50 seconds physical time, the interval of 1 second shows a better load balance than interval of 2 seconds. However, for the simulation of 50 - 100 seconds, interval of 2 seconds is better. This implies that loss of load balance accumulates faster and requires a more frequent re-decomposition of domain at the early stages of simulation. This is consistent with the plume development process: the domain grows quickly at the beginning as erupted material ejects into the environment and spreads. Afterwards, the spreading speed of the front edge slows down due to viscosity and momentum exchange leading to slowing down of domain growth. We need to emphasize that, these optimized parameters, including weights of particles and load balance checking interval, are case sensitive and need to be re-calibrated for different applications and hardware architecture.

\subsection{Effect of dynamic halo domains} \label{sec:effect-of-halo-domain}

\begin{table}
\centering
{\caption{Computational workload of extra steps for domain adjusting. SWCH represents the step that switch pressure ghost particle to real particle, ADPP is short for adding new pressure ghost particles, ADWP represents adding wall ghost particles, SCN is short for scanning the outmost layer of the domain.  Momentum and energy update (UPME) and position update (UPP) also included for comparison.}  
    \begin{tabular}{lrr}
    \hline
    Functions & Total time (s) & Called times\\
    	\hline
    UPME & 2954.8 & 201 \\
    UPP & 38.55 &  201 \\
    ADPP & 21.51 & 3 \\
    ADWP  & 8.88 & 3 \\
    SWCH & 0.08 &  2 \\
    SCN  & 7.72 & 201 \\
    \hline
   \end{tabular}
\label{tab:Computational_cost_doamin_adj}}
\end{table}

\begin{figure}
\centering
{\includegraphics[scale=0.5]{Chapter-5/Figures/adj_vs_no}}
{\caption{The figure on the top shows execution time without using the dynamic halo domain algorithm, the figure on the bottom shows execution time using the dynamic halo domain algorithm. Different bins represent execution time up to specific physical time indicated by horizontal axis.}
\label{fig:adj_vs_no}}
%\end{floatrow}
\end{figure}

For the test problem, the volcano plume finally reaches to a region of $[-10km \,\,\, 10km] \times [-10km\,\,\,10km] \times [0km\,\,\,20km]$ after around 300 seconds of eruption. When numerical simulation goes up to 90 seconds, the plume is still within a region of $[-3km\,\,\,3km] \times [-3km\,\,\,3km] \times [0km\,\,\,6km]$. This implies that adjusting of domain can avoid computing large number of uninfluenced particles, especially for the early stages of simulation. On the other hand, additional functions (see Fig. \ref{fig:Work_flow}) for domain adjusting will add extra computational cost. Table \ref{tab:Computational_cost_doamin_adj} shows the extra computational cost corresponding to these functions. Profiling data for momentum and energy update (UPME) and position update (UPP) is also included in the table for the purpose of comparison. As we can see from the table, the cost of SCN is even smaller than UPP function, which is the cheapest function (as shown in table \ref{tab:Computational_cost_steps}) in the regular workflow (see Fig. \ref{fig:Work_flow}). The other three functions, ADPP, ADWP and SWCH, are only called a few times during the simulation, and as a consequence, the extra computational cost due to them are negligible. To summarize, the additional cost caused by the domain adjusting functions is ignorable.
Figure \ref{fig:adj_vs_no} shows that simulation time of the test problem is greatly reduced when we adopt the domain adjusting strategy in our code.

\section{Conclusion}
We developed data management strategies for a MPI-parallel implementation of the SPH method to simulate volcano plumes. Neighbor searching and domain decomposition is based on the background grid. A SFC based index scheme is adopted to identify buckets and time-dependent SFC keys are used as identifiers for particles. 
Hash tables with external linked lists are adopted for accessing particles and buckets data. Based on calibrated particle weights, a dynamic load balance strategy is developed by checking load balance and re-decomposing the domain at an optimized interval. The performance of the code was further improved by several factors by adjusting the computational domain during simulation.
Overall we obtain good scalability and performance.

The flexibility of our data access methodology enables implementing mesh-free methods for solving more complicated problems and using more advanced techniques, such as dynamic particle splitting techniques \citep{vacondio2012accurate, feldman2007dynamic}, which will give higher resolution at the area of interest by splitting one large particle to several smaller ones. The data structure, particle and bucket indexing strategies, domain decomposition, dynamic load balancing method and domain adjusting strategies in present work can be adopted to other mesh-free methods (not just SPH).
