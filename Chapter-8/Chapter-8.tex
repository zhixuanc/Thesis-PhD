%SourceDoc ../YourName-Dissertation.tex
\chapter{Conclusion and Future Work} \label{chapter:Future-Work}

\section{Conclusion remarks}
A new plume model was developed based on the SPH method adopting dusty-gas 1.5 phases compressible turbulent flow governing equations. Extensions necessary for Lagrangian methodology and compressible flow were made in the formulation of the equations of motion and turbulence models. Advanced numerical techniques in SPH were exploited, tailored and developed for this model. High performance computing was used to mitigate the tradeoff between accuracy (depends on comprehensiveness of the model, resolution, order of accuracy of numerical methods, scheme for time upgrading.) and simulation time (depends on comprehensiveness of model, resolution, order of accuracy of numerical methods, scheme for time upgrading and computational techniques). The correctness of the code was verified by a series of test simulations, in which simulation results are compared against analytical solution and experimental results. The final outcome of the work, an open source software named Plume-SPH, was used for a case study, the Pinatubo eruption at June 15th 1991. The case study not only validated the assumptions made to simplify the physics but also these numerical techniques we adopted. Plume-SPH is then combined with PUFF for ash transportation and dispersion forecasting, taking June 15th 1991 Pinatubo eruption as the case, demonstrating its merit in improving accuracy of volcanic ash transportation forecasting.

Currently existing 3D models focus on certain aspect of the volcanic plume and hence, naturally, different assumptions were made in these models. However, these different aspects of volcanic plumes are not independent, they are actually coupled. For example, it has been illustrated by \cite{cerminara2016large} that gas-particle non-equilibrium would introduce a previously unrecognized jet-dragging effect, which imposes great influence on plume development, especially for weak plumes. In addition, there is no absolute boundary to determine which kind of hazard is dominant in certain eruptions. So it is necessary to simulate all associated hazards in one model. Actually, effort has already been put on developing more comprehensive plume models. For example, a large-particle module (LPM) was added to ATHAM to track the paths of rocky particles (pyroclastic or tephra) within the plume and predict where these particles fall \citep{kobs2009modeling}. We were also motivated by such an evolution of plume modeling to choose SPH as our numerical tool. The good extensibility of SPH method, which allows adding new physics and phases with much less modification of the code compared with mesh based methods, adds extra merit to Plume-SPH. That is, Plume-SPH is supposed to be more sustainable.
Last but not least, the dramatic development of computational power makes it possible to establish a more comprehensive model. While current computational capacity may not allow us to have a fully comprehensive model, the easy-extension feature of SPH makes it convenient to keep adding new physics into the model when necessary and computationally feasible. 

Work presented in this thesis is initial efforts and results towards developing a first principle based plume model with comprehensive physics, adopting proper numerical tools and high performance computing. As will be discussed in next section, more comprehensive physics model, advanced numerical techniques and better data management strategies and algorithms are on our list to exploit in the future. Our code will also be made available in the open source form for the community to enhance.

\section{Improvement of the software}
The research presented in this thesis developed the first version of Plume-SPH.
My proposal for future improvement of the software includes four aspects: physics model, numerical techniques, software performance and software usability.

\subsection{Physics model}
The highest priority of the future research will be given to improvement on the physics model. Simplification based on assumptions facilitated the development of the software from all sides. On the other hand, these assumptions also impair the prediction capability of the software. The first assumption that should be removed is the stationary atmosphere assumption, which gets rid of wind field from the model. Without accounting for the effect of the wind fields, the software can only be used to simulate strong plume, for which the effects of wind fields is ignorable. It is straightforward to include wind field in physical model. No changes in governing equations are needed. Only the pressure outlet boundary condition needs to be changed. More challenges will come from numerical techniques for imposing the new types of boundary conditions in SPH. 
%There are few researches on such topic up to date, let alone successful implementations in real complicated phenomena simulation. Besides the stationary atmosphere assumption,
The immediate dynamics and thermal dyanmic equilibrium assumption can be removed, too. In that case, each phase (air and erupted material) will be described by a full set of mass conservation equation, momentum equations and energy equation. Removing of such assumption would lead to pressure difference on two sides of interface. techniques to handle the pressure difference will be necessary in that case. Another assumption, which assumes that the erupted material is well mixed and behaves like a single phase fluid, needs to be removed if more subtle phenomena are of interest or play more dominant role. Among these potential improvements, including of wind field is the most urgent matter.

Several resources can be made use of as references. PDAC\citep{neri2003multiparticle} adopts true multiphase governing equations and might serve as a good point to get start to remove the ``well mixed" assumption. ATHAM  \citep{oberhuber1998volcanic}, which accounts for several microphysics in the model, would be a good reference for including more microphysics. 1D plume models \citep{bursik2001effect, pouget2016sensitivity, folch2016fplume} are now pretty comprehensive, hence conclusions based on 1D plume simulation would also provide merit for more comprehensive 3D plume development.

\subsection{Numerical techniques}
SPH, even though is considered as a promising method, is still not considered as a serious candidate to become tomorrow's numerical tool. One of the main reason of this is that SPH still has unknown properties, and many questions remain unanswered on a purely theoretical ground, such as convergence, numerical stability, boundary conditions, kernel properties, time marching, existence and properties of solutions.
Adopting of more advanced numerical techniques will greatly depend on progress in purely theoretical understanding of SPH. 
Besides these more general aspects, accuracy and prediction capability of Plume-SPH can be improved if the following numerical techniques can be implemented successfully. 
\begin{itemize}
\item A better way to handle ``mixing issue". Mixing plays a fundamental rule in volcanic plume development. As has been discussed in the first chapter, classical SPH has problems in correctly integrating fluid instabilities and mixing at boundaries \citet{read2010resolving}. There are different opinions on the sources of the ``mixing issue" and different strategies to handle this issue among researchers \citep{chen1999improvement, ritchie2001multiphase, agertz2007fundamental, wadsley2008treatment, price2008modelling, read2010resolving, borgani2012hydrodynamic}. A better understanding on ``mixing issue" and new techniques to tackle ``mixing issue" is critical for improving accuracy of the software.
\item Adaptive resolution technique. Adaptive SPH particles size \citep{lopez2013dynamic, vacondio2016variable} is one of the advanced numerical techniques that might be worthwhile to explore. Benefits of successfully implementing such techniques are multiple folders. Number of particles (the discretized points) can be reduced by assigning fine resolution only when necessary and hence computational cost could be reduced. Accuracy could be increased in refined areas such as interface between volcano plume and surrounding atmosphere. Losing of strict conservation of momentum and energy caused by unequal particle mass could also be relieved by adopting the adaptive particle size scheme. The algorithm for particle splitting has to specify the number of split (daughter) particles, their distribution, spacing and kernel size. Vice verse for particle coalescing.
\item Turbelence model. A LANS type of turbulence model is adopted and extended in Plume-SPH. It would be interesting (maybe necessary) to compare the effects of using different types of turbulence models or just varying parameters $SPH-\epsilon$ turbulence model.
\item Higher order SPH, in terms of both space approximation and time integration, might worthwhile to consider. The time integration scheme in current software is first order Euler method. Even though relatively scarce, there have been researches \citep{blanc2012stabilized} on higher order time integration in SPH and it was reported that higher order time integration can avoid the tensile instability issue. There are more attempts on higher order spatial approximations \citep{bonet1999variational, dilts1999moving, leonardi2014explicit, lind2016high}.
\item More work on RSPH. HLLC approximate Riemann solver is used in test investigation in Chapter \ref{chapter:GSPH-RSPH}. There are several other variations can be attempted, for example, alternative Riemann solvers, alternative ways for constructing Riemann problems and alternative ways for sampling solution of Riemann problem. In addition, the RSPH method, with the capability of introducing less and adaptive numerical dissipation, is supposed to be a good candidate for handling the ``mixing issue". More benchmark investigations (such as RTI tests or KHI tests \citep{price2008modelling,cha2010kelvin}) regarding RSPH's capability of handling turbulent mixing are necessary.
\end{itemize}

\subsection{Computational efficiency and parallel scalability}
The computational efficiency and scalability of the software is enough for implementations presented in Chapter 6 and Chapter 7. However future extending of the software by including more physics and phases would impose higher requirement. Adopting of new numerical techniques could also raise new challenges. In addition, even for other applications, current computational efficiency is not enough. For example, the attempt of simulating umbrella stage of volcano plume development was failed as it involves a much larger computational domain and much longer physical time. The following strategies might be worthwhile to attempt to handle these challenges and demands. 

\begin{itemize}
\item A better way to handle hash conflict. Current way for handling hash conflict is adding a linked list, which can increase memory accessing time dramatically when the list is long. Extra time spending on memory accessing would lead to increase of workload of these particles in the linked list, results in serious load imbalance. Alternative ways with lower memory access cost, for handle hash conflictions, would also be helpful, for example, an external set instead of linked list, an external hash table or re-hashing based on more recent particle location.
\item A better domain decomposition strategy is worthwhile to try. Current SFC based domain decomposition strategy pays enough attention to load balance. It also preserves spatial locality. However, no constraint is included in the algorithm to minimize overlapped particles (between subdomains) and hence minimizing communications between processors. Either avoiding splitting domain through high-particle-density area or reducing "length" of interface between subdomains could reduce amount of information that needs to be communicated. What's more, the dynamic halo domain algorithm might lead to heavy communication overhead due to irregular shape of computational domain. Extra attention should also be paid to this issue.
\item Less zigzag memory access. The current basic data structure adopts indirect memory access pattern. To access physical quantities associated with a particle, the pointer to the HashEntry is first found through hash function and then the pointer to the particle is founded based on HashEntry. Finally the physical quantity will be access through pointers to particle objects. That is to say, to load physical quantities of a particles, the memory needs to be accessed for several times.
So the time spend on memory access (majorly time on waiting for data loading from memory) might be much more than that spend on computation attribute to such zigzags memory access pattern. Eliminating of one or two intermediate pointers is highly desired providing the architecture and maintainability of the software dose not compromise. Integrating all physical updating steps into one function would reduce computational cost dramatically by reducing intermediate calculations and memory access. However, this strategy would hurt the maintainability of code and should be avoided.
\item Hybrid parallelization. As the capacity of " memory access buses" is the bottle neck of current code, hybrid parallelization that combines shared memory mode and distributed memory mode would not increase the computational efficiency as expected. However, after adopting a less zigzags memory access pattern, the hybrid parallelization can greatly increase computational efficiency and scalability due to significant decrease in communication.
\item Any efforts to take advantage of heterogeneous platforms and tackle challenges related to diversity and heterogeneity of hardware are necessary as well.
\end{itemize}

\subsection{Usability}
The usability of the software could be improved in several aspects, for example, putting all input parameters in a single file, or at lease, avoiding re-compile if input parameters changed. Several utilities should be provided for determining smoothing lengths, particle masses and domain specifications. Determination of these parameters are tricky and requires good knowledge on SPH method and details about the source code.
A wrap up around the solvers, which could provide well-defined user interface, is highly desirable for users purely interested in application study. Post processing in this thesis was carried out based on some low-efficient Paraview-based python/matlab scripts, which requires proficiency in Paraview, python and matlab to use. An efficient and user-friendly post processing module is necessary, too. At the same time, a more flexible and user-friendly pre-process module is necessary as well. 

\section{Future implementations}
Besides improving the software, more implementations are desired. These implementations will also help to identify hidden bugs, defects and new demands. As a results, drive further development of Plume-SPH.
We propose several potential implementations in the following.
\begin{itemize}
\item Simulating of other strong eruptions. The current model is suitable for simulating eruptions in which wind field, large size solid particles and micro-physics play ignorable role during plume development. It is also desired that there are enough observational data available for case study. One eruption at hand is the second phase of 2010 eruptions of Eyjafjallajökull. However, the eruption conditions of Eyjafjallajökull varied in a large range leading to top heights varying in a large range. What's worse, besides the bent effect of wind field, the quick cooling down and extra water fraction due to melting of ice might not able to properly accounted for by current model.
\item Eruption condition sensitivity study. Sensitivity studies with respect to eruption parameters have been conducted based on other 1D and 3D plume models. However, it is still valuable to confirm their conclusions with a new model and test the capacity of this new model.
\item Sensitivity studies on other aspects of the volcano eruption. For example, uniform and parabolic eruption velocity profiles are readily available in the software. There are also four different atmosphere types available: uniform, purely hydrostatic, approximation model based on observation, realistic atmosphere based on input data. Sensitivity study regarding these aspects can be easily done.
\item Combining the plume development model with other VATDs. A combination of Plume-SPH with one of the VATDs, PUFF, is implemented in this thesis. However, more comprehensive 3D VATDs are available. It might be worthwhile to combine the 3D plume model with more comprehensive 3D VATDs. In addition, considering the plume height is varying as a function of time, it would be more accurate to use time-dependent eruption conditions.
\item Combining the plume model with underground magma reservoir models. Plume development simulations in present work are based on eruption parameters that are obtained from post-eruption estimations. An alternative way is combining the plume model development model with magma reservoir models that can predict eruption conditions.
\item Integrating the current plume model into existing uncertainty and data integration framework for volcanic ash hazards forecasting \citep{patra2013challenges, madankan2014computation, stefanescu2014temporal}. Uncertainties in volcaninc plume modeling and ash transportation forecast would not be eliminated by comprehensive first-principle based 3D plume model. Dynamic eruption condition estimation is always necessary when modeling real eruptions, during which eruption conditions changes along time. However, the computational efficiency has to be improved first. The current computational speed is not fast enough to support either uncertainty quantification nor data integration.
\end{itemize}